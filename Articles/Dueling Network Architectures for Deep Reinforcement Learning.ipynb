{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: ICML\n",
    "\n",
    "Year: 2016\n",
    "\n",
    "Authors: Ziyu Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, Nando de Freitas\n",
    "\n",
    "Institutions: DeepMind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dueling architecture, explicitly separates the representation of state values and (state-dependent) action advantages. The dueling architecture consists of two stream that represent the value and advantage functions, while sharing a common convolutional feature learning module. The two streams are combined via a special aggregating layer to produce an estimate of the state-action value function $Q$. The dueling network automatically produces separate estimates of the state value function and advantage function, without any extra supervision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, the dueling architecture can learn which states are (or are not) valuable, without having to learn the effect of each action for each state. This is particularly useful in states where its actions do not affect the environment in any relevant way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantage function, relating to value functions:\n",
    "$$A^\\pi(s,a)=Q^\\pi(s,a)-V^\\pi(s,a)$$\n",
    "Note that $E_{a\\sim\\pi(s)}[A^\\pi(s,a)]=0$. Intuitively, the value function $V$ measures how good it is to be in a particular state $s$. The $Q$ function, however, measures the value of choosing a particular action when in this state. The advantage function subtracts the state value from $Q$ to obtain a relative measure of the importance of each action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key insight behind the dueling architecture is that for many states, it is unnecessary to estimate the value of each action choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of following the convolutional layers with a single sequence of fully connected layers, we instead use two sequences (or streams) of fully connected layers. The streams are constructed such that they have the capability of providing separate estimates of the value and advantage functions. Finally, the two streams are combined to produce a single output $Q$ function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive equation of $Q$ is unidentifiable in the sense that given $Q$ we cannot recover $V$ and $A$ uniquely. This lack of identifiability is mirrored by poor practical performance. \n",
    "\n",
    "To address the issue of identifiability, force the advantage function estimator to have zero advantage at the chosen action.\n",
    "$$Q(s,a;\\theta,\\alpha,\\beta)=V(s;\\theta,\\beta)+(A(s,a;\\theta,\\alpha)-\\max_{a'\\in|\\mathcal{A}|}A(s,a';\\theta,\\alpha))$$\n",
    "An alternative module replaces the max operator with an average:\n",
    "$$Q(s,a;\\theta,\\alpha,\\beta)=V(s;\\theta,\\beta)+(A(s,a;\\theta,\\alpha)-\\frac{1}{|\\mathcal{A}|}\\sum_{a'}A(s,a';\\theta,\\alpha))$$\n",
    "On the one hand this loses the original semantics of $V$ and $A$ because they are now off-target by a constant, but on the other hand it increases the stability of the optimization: the advantages only need to change as fast as the mean, instead of having to compensate any change to the optimal actionâ€™s advantage. Softmax version delivers similar results to the simpler module of average. Hence, all the experiments use the average module."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
