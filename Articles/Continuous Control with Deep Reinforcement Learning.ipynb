{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: ICLR\n",
    "\n",
    "Year: 2016\n",
    "\n",
    "Authors: Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, Daan Wierstra\n",
    "\n",
    "Institutions: Google DeepMind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Deep Q Network solves problems with high-dimensional observation spaces, it can only handle discrete and low-dimensional action spaces. It can’t be straight-forwardly applied to continuous domains since it relies on a finding the action that maximizes the action-value function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DPG algorithm maintains a parameterized actor function $\\mu(s|\\theta\\mu)$ which specifies the current policy by deterministically mapping states to a specific action. The critic $Q(s, a)$ is learned using the Bellman equation as in Q-learning. The actor is updated by following the applying the chain rule to the expected return from the start distribution $J$ with respect to the actor parameters:\n",
    "$$\\nabla_{\\theta_\\mu}J=E_{s_t\\sim\\rho^\\beta}[\\nabla_aQ(s,a|\\theta^Q)|_{s=s_t,a=\\mu(s_t)}\\nabla_{\\theta_\\mu}\\mu(s|\\theta^\\mu)|_{s=s_t}]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directly implementing Q-Learning with neural networks proved to be unstable in many environments. Since the network $Q(s,a|\\theta^Q)$ being updated is also used in calculating the target value, the Q update is prone to divergence. Rather than directly copying the weights, create a copy of the actor and critic networks, $Q'(s,a|\\theta^{Q'})$ and $\\mu'(s|\\theta^{\\mu'})$ respectively, that are used for calculating the target values. The weights of these target networks are then updated by having them slowly track the learned networks: $\\theta'=\\gamma\\theta+(1-\\gamma)\\theta'$ with $\\gamma << 1$. This means that the target values are constrained to change slowly, greatly improving the stability of learning. This simple change moves the relatively unstable problem of learning the action-value function closer to the case of supervised learning, a problem for which robust solutions exist. Having both a target $\\mu'$ and $Q′$ was required to have stable targets $y_i$ in order to consistently train the critic without divergence. This may slow learning, since the target network delays the propagation of value estimations. However, in practice this was greatly outweighed by the stability of learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When learning from low dimensional feature vector observations, the different components of the observation may have different physical units and the ranges may vary across environments. This can make it difficult for the network to learn effectively and may make it difficult to find hyper-parameters which generalise across environments with different scales of state values.\n",
    "\n",
    "It is solved by adapting Batch Normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A major challenge of learning in continuous action spaces is exploration. An exploration policy is constructed by adding noise sampled from a noise process to the actor policy. The process can be chosen to suit the environment. Ornstein-Uhlenbeck process is chosen to generate temporally correlated exploration for exploration efficiency in physical control problems with inertia."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
