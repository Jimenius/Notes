{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: NIPS\n",
    "\n",
    "Year: 2013\n",
    "\n",
    "Authors: Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin Riedmiller\n",
    "\n",
    "Institutions: DeepMind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement Learning algorithms must be able to learn from a scalar reward signal that is frequently sparse, noisy, and delayed.\n",
    "\n",
    "Most Deep Learning algorithms assume the data samples to be independent, while in Reinforcement Learning one typically encounters sequences of highly correlated states.\n",
    "In Reinforcement Learning, the data distribution changes as the algorithm learns new behaviors, which can be problematic for Deep Learning methods that assume a fixed underlying distribution.\n",
    "\n",
    "To alleviate the problems of correlated data and non-stationary distributions, an experience replay mechanism, which randomly samples previous transitions and thereby smooths the training distribution over many past behaviors, is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Q-network can be trained by minimising a sequence of loss functions $L_i(\\theta_i)$ that changes at each iteration $i$,\n",
    "$$L_i(\\theta_i)=E_{s,a\\sim\\rho(\\cdot);s'\\sim\\mathcal{E}}[(y_i-Q(s,a;\\theta_i))^2]$$\n",
    "where $y_i=E_{s\\sim\\mathcal{E}}[r+\\gamma\\max_{a'}Q(s',a';\\theta_{i-1})]$ is the target for iteration $i$ and $\\rho(s,a)$ is a probablility distribution over sequences $s$ and actions $a$ as behaviour distribution. The parameters from the previous iteration $\\theta_{i-1}$ are held fixed when optimising the loss function $L_i(\\theta_i)$. The targets depend on the network weights; this is in contrast with the targets used for supervised learning, which are fixed before learning begins. Differentiating the loss function with respect tot the weights:\n",
    "$$\\nabla_{\\theta_i}L_i(\\theta_i)=E_{s,a\\sim\\rho(\\cdot);s'\\sim\\mathcal{E}}[(r+\\gamma\\max_{a'}Q(s',a';\\theta_{i-1})-Q(s,a;\\theta_i))\\nabla_{\\theta_i}Q(s,a;\\theta_i)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experience replay stores the agentâ€™s experiences at each time-step, pooled over many episodes into a replay memory. The advantages are:\n",
    "1. Each step of experience is potentially used in many weight updates, which allows for greater data efficiency.\n",
    "2. Learning directly from consecutive samples is inefficient, due to the strong correlations between the samples.\n",
    "3. Randomizing the samples breaks these correlations and therefore reduces the variance of the updates.\n",
    "4. The behavior distribution is averaged over many of its previous states, smoothing out learning and avoiding oscillations or divergence in the parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
