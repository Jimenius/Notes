{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">Reinforcement Learning: An Introduction</span>\n",
    "Authors: Richard Sutton, Andrew Barto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Chapter 1 Introduction</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:green\">1.1 Reinforcement Learning</span>\n",
    "Reinforcement Learning is learning what to do - how to map situation to actions - so as to maximize a numerical reward signal. \n",
    "\n",
    "Reinforcement Learning VS Supervised Learning: Supervised Learning is not adequate for learning from interaction.\n",
    "\n",
    "Reinforcement Learning VS Unsupervised Learning: Unsupervised Learning by itself does not address the Reinforcement Learning problem of maximizing a reward signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:green\">1.3 Elements of Reinforcement Learning</span>\n",
    "Policy: Agent’s way of behaving at a given time\n",
    "\n",
    "Reward: How good or bad events for the agents\n",
    "\n",
    "Value function: What is good in the long run\n",
    "\n",
    "Model: Given a state and an action, the model might predict the resultant next state and next reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:green\">1.4 Limitations and Scopes</span>\n",
    "Evolutionary method: The policies that obtain the most reward, and random variations of them, are carried over to the next generation of policies, and the process repeats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Chapter 2 Multi-armed Bandits</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:green\">2.1 A $k$-armed Bandits</span>\n",
    "Value of an action:\n",
    "$$q_*(a)=E(R_t|A_t=a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:green\">2.2 Action-value Methods</span>\n",
    "Greedy action selection: Select one of the actions with the highest estimated value.\n",
    "\n",
    "$\\epsilon$-Greedy: Behave greedily most of the time, but with small probability select randomly from all the actions with equal probability, independently of the action-value estimates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:green\">2.3 The 10-armed Testbed</span>\n",
    "The Greedy method improves slightly faster than the other methods at the very beginning but then leveled off at a lower level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:green\">2.4 Incremental Methods</span>\n",
    "$$NewEstimate\\leftarrow OldEstimate+StepSize(Target-OldEstimate)$$\n",
    "The $(Target-OldEstimate)$ is an error in the estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:green\">2.5 Tracking a Nonstationary Problem</span>\n",
    "Fix step-size for nonstationary (reward probabilities change overtime) problem.\n",
    "\n",
    "Convergence conditions for step-size function ($\\alpha$):\n",
    "$$\\sum_{n=1}^\\infty \\alpha_n(a)=\\infty$$\n",
    "$$\\sum_{n=1}^\\infty \\alpha_n^2(a)=0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:green\">2.6 Optimistic Initial Values</span>\n",
    "Algorithms are usually dependent to some extent on the initial values. Initial values introduce bias.\n",
    "\n",
    "Optimistic initial values:\n",
    "- Downside: Initial estimates become a set of parameters picked by the user.\n",
    "- Upside: Easy way to supply prior knowledge.\n",
    "\n",
    "Optimistic initial values encourage exploration, and are more efficient on stationary problems, but far from a useful approach to encourage exploration, especially in non-stationary problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:green\">2.7 Upper-Confidence-Bound Action Selection</span>\n",
    "$\\epsilon$-greedy is indiscriminate, with no preference for those nearly greedy choices.\n",
    "Upper-Confidence-Bound Action Selection (UCB):\n",
    "$$A_t=argmax_a(Q_t(a)+c\\sqrt \\frac{ln(t)}{N_t(a)})$$\n",
    "$N_t(a)$ denotes the number of times that action a has been selected prior to time t, $c$ controls the degree of exploration. If $N_t(a)=0$, then $a$ is considered to be a maximizing action.\n",
    "\n",
    "Actions with lower value estimates, or that have already been selected frequently, will be selected with decreasing frequency over time.\n",
    "\n",
    "UCB performs better in Multi-arms Bandits\n",
    "\n",
    "UCB is difficult to extend to more general RL problems (non-stationary and large state spaces)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:green\">2.8 Gradient Bandit Algorithm</span>\n",
    "Softmax Distribution:\n",
    "$$\\pi_t(a)=Pr(A_t=a)=\\frac{e^{H_t(a)}}{\\sum_{b=1}^k e^{H_t(b)}}$$\n",
    "Gradient Bandit Algorithms with softmax distribution:\n",
    "\n",
    "After selecting an action:\n",
    "$$H_{t+1}(A_t)=H_t(A_t)+\\alpha(R_t-\\bar{R_t})(1-\\pi_t(A_t))$$\n",
    "$$H_{t+1}(a)=H_t(a)+\\alpha(R_t-\\bar{R_t})\\pi_t(a),\\forall a\\neq A_t$$\n",
    "Gradient Bandit learns action preference instead of action value. Action preferences are initially set to be the same so that all actions have an equal probability of being selected. It selects action with softmax. Action preferences are updated. If the reward is higher than the baseline, the probability of taking that action in the future is increased. If the reward is below the baseline, the probability of taking that action in the future is decreased. The non-selected actions move in the opposite direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:green\">2.9 Associate Search</span>\n",
    "Associative searching (Contextual Bandit) tasks associate different actions with different situations. It intermediates between the k-armed bandit problem and the full reinforcement learning problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Chapter 3 Finite Markov Decision Processes</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:green\">3.1 The Agent=Envionment Interface</span>\n",
    "In a Markov Decision Process, the probabilities completely characterize the environment’s dynamics. The probability of each possible value for state and reward depends only on the immediately preceding state and action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:green\">3.3 Returns and Episodes</span>\n",
    "Return:\n",
    "$$G_t=R_{t+1}+R_{t+2}+\\cdots+R_T$$\n",
    "with discounting:\n",
    "$$G_t=R_{t+1}+\\gamma R_{t+2}+\\cdots=\\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}$$\n",
    "Terminal State: Episode ends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:green\">3.5 Policies and Value Functions</span>\n",
    "Policy: A mapping from states to probabilities of selecting each possible action.\n",
    "\n",
    "Value function, Action-value function: Expected return given state (or state-action pair)\n",
    "$$v_\\pi(s)=E_\\pi(G_t|S_t=s)=E_\\pi(\\sum_{k=0}^\\infty\\gamma^k R_{t+k+1}|S_t=s)$$\n",
    "$$q_\\pi(s,a)=E_\\pi(G_t|S_t=s,A_t=a)=E_\\pi(\\sum_{k=0}^\\infty\\gamma^k R_{t+k+1}|S_t=s,A_t=a)$$\n",
    "Bellman Equation:\n",
    "$$v_\\pi(s)=\\sum_a\\pi(a|s)\\sum_{s',r}p(s',r|s,a)(r+\\gamma v_\\pi(s')),\\forall s\\in S$$\n",
    "Bellman Equation expresses a relationship between the value of a state and the values of its successor states.\n",
    "\n",
    "Block Diagram diagrams relationships that form the basis of the update or backup operations that are at the heart of reinforcement learning methods. Open circle represents a state. Solid circle represents a state-action pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:green\">3.6 Optimal Policies and Optimal Value Functions</span>\n",
    "A policy is defined to be better than or equal to another policy if and only if the values are no smaller than that of the other for all states.\n",
    "\n",
    "Optimal policy:\n",
    "$$v_*(s)=max_\\pi v_\\pi(s)$$\n",
    "$$q_*(s,a)=max_\\pi q_\\pi(s,a)$$\n",
    "There is always at least one policy that is better than or equal to all other policies.\n",
    "Although there may be more than one optimal policies. They share the same state value functions.\n",
    "\n",
    "Bellman Optimal Equation:\n",
    "$$v_*(s)=max_a\\sum_{s',r}p(s',r|s,a)(r+\\gamma v_*(s')),\\forall s\\in S$$\n",
    "$$q_*(s,a)=\\sum_{s',r}p(s',r|s,a)(r+\\gamma max_{a'}q_*(s',a')),\\forall s\\in S,a\\in A$$\n",
    "For finite MDP, the Bellman optimality equation for optimal policy has a unique solution.\n",
    "\n",
    "Explicitly solving the Bellman optimality equation provides one route to finding an optimal policy, and thus to solving the reinforcement learning problem. However, this solution is rarely directly useful. It is akin to an exhaustive search, looking ahead at all possibilities.\n",
    "\n",
    "The solution Relies on three assumptions:\n",
    "- The dynamics are known.\n",
    "- Enough computational resources to complete the computation.\n",
    "- Markov property"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Chapter 4 Dynamic Programming</span>\n",
    "Dynamic Programming solution to Reinforcement Learning assumption:\n",
    "- Finite MDP\n",
    "- Dynamics are completely known"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:green\">4.1 Policy Evaluation (Prediction)</span>\n",
    "Iterative Policy Evaluation update rule:\n",
    "$$v_{k+1}(s)=\\sum_a\\pi(a|s)\\sum_{s',r}p(s',r|s,a)(r+\\gamma v_k(s')),\\forall s\\in S$$\n",
    "The sequence {$v_k$} can be shown in general to converge to $v_\\pi$ as $k\\to\\infty$ under the same conditions that guarantee the existence of $v_\\pi$.\n",
    "\n",
    "Formally, iterative policy evaluation converges only in the limit, but in practice it must be halted short of this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:green\">4.2 Policy Improvement</span>\n",
    "If it is better to select $a$ once in $s$ and thereafter follow $\\pi$ than it would be to follow $\\pi$ all the time, then one would expect it to be better still to select $a$ every time $s$ is encountered, and that the new policy would in fact be a better one overall.\n",
    "\n",
    "Policy Improvement Theorem:\n",
    "\n",
    "Let $\\pi$ and $\\pi'$ be any pair of deterministic policies such that,\n",
    "$$q_\\pi(s,\\pi'(s))\\geq v_\\pi(s),\\forall s\\in S$$\n",
    "Then the policy $\\pi'$ must be as good as, or better than, $\\pi$.\n",
    "\n",
    "With policy improvement, when the new policy is equal to the old policy, both policies must be optimal, as explained by Bellman Optimality Equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:green\">4.4 Value Iteration</span>\n",
    "One drawback to policy iteration is that each of its iterations involves policy evaluation, which may itself be a protracted iterative computation requiring multiple sweeps through the state set.\n",
    "\n",
    "Value Iteration update rule:\n",
    "$$v_{k+1}(s)=max_a\\sum_{s',r}p(s',r|s,a)(r+\\gamma v_k(s')),\\forall s\\in S$$\n",
    "The sequence {$v_k$} can be shown in general to converge to $v_*$ as $k\\to\\infty$ under the same conditions that guarantee the existence of $v_*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:green\">4.5 Asynchronous Dynamic Programming</span>\n",
    "A major drawback to the DP methods is that they involve operations over the entire state set of the MDP, that is, they require sweeps of the state set.\n",
    "\n",
    "Asynchronous DP\n",
    "Sweep part of the states in every iteration instead of all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:green\">4.6 Generalized Policy Iteration</span>\n",
    "In policy iteration, policy evaluation and policy improvement alternate, each completing before the other begins, but it is not necessary.\n",
    "\n",
    "GPI (Generalized Policy Iteration) refers to the general idea of letting policy evaluation and policy-improvement processes interact.\n",
    "If both the evaluation process and the improvement process stabilize, then the value function and policy must be optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:green\">4.7 Efficiency of Dynamic Programming</span>\n",
    "DP may not be practical for very large problems, but compared with other methods for solving MDPs, DP methods are efficient. The worst case time DP methods take to find an optimal policy is polynomial in the number of states and actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Chapter 5 Monte Carlo Methods</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:green\">5.1 Monte Carlo Prediction</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:green\">5.2 Monte Carlo Estimation of Action Value</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:green\">5.3 Monte Carlo Control</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:green\">5.4 Monte Carlo Control without Exploring Starts</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:green\">5.5 Off-policy Prediction with Importance Sampling</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:green\">5.6 Incremental Implementation</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:green\">5.7 Off-policy Monte Carlo Control</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Chapter 6 Temporal Difference Learning</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:green\">6.1 TD Prediction</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:green\">6.2 Average of TD Prediction Methods</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:green\">6.3 Optimality of TD(0)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:green\">6.4 SARSA: On-Policy TD Control</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Chapter 8 Planning and Learning with Tabular Methods</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:green\">8.1 Models and Planning</span>\n",
    "Anything that an agent can use to predict how the environment will respond to its actions is a model of the environment.\n",
    "\n",
    "Distribution models produce a description of all possibilities and their probabilities. \n",
    "\n",
    "Sample models produce just one of the possibilities, sampled according to the probabilities.\n",
    "\n",
    "Distribution models are stronger than sample models in that they can always be used to produce samples. However, in many applications it is much easier to obtain sample models than distribution models.\n",
    "\n",
    "Planning refers to any computational process that takes a model as input and produces or improves a policy for interacting with the modeled environment.\n",
    "\n",
    "State-space planning is viewed primarily as a search through the state space for an optimal policy or an optimal path to a goal. Actions cause transitions from state to state, and value functions are computed over states.\n",
    "\n",
    "Plan-space planning is instead a search through the space of plans. Operators transform one plan into another, and value functions, if any, are defined over the space of plans. Plan-space planning includes evolutionary methods and partial-order planning, in which the ordering of steps is not completely determined at all stages of planning. Plan-space methods are difficult to apply efficiently to the stochastic sequential decision problems that are the focus in reinforcement learning.\n",
    "\n",
    "The unified view is that all state-space planning methods share a common structure, a structure that is also present in the learning methods.\n",
    "1. All state-space planning methods involve computing value functions as a key intermediate step toward improving the policy.\n",
    "2. Value functions are computed by updates or backup operations applied to simulated experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:green\">8.2 Dyna: Integrated Planning, Acting, and Learning</span>\n",
    "Direct reinforcement learning uses real experience to directly improve the value function and policy.\n",
    "\n",
    "Model learning (indirect reinforcement learning) uses real experience to improve the model (to make it more accurately match the real environment)\n",
    "\n",
    "Indirect methods often make fuller use of a limited amount of experience and thus achieve a better policy with fewer environmental interactions.\n",
    "\n",
    "Direct methods are much simpler and are not affected by biases in the design of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:green\">8.4 Prioritized Sweeping</span>\n",
    "In the Dyna agents, simulated transitions are started in state–action pairs selected uniformly at random from all previously experienced pairs. But a uniform selection is usually not the best; planning can be much more efficient if simulated transitions and updates are focused on particular state–action pairs.\n",
    "\n",
    "It is pointless to perform updates along almost all transitions, because they take the agent from one zero-valued state to another, and thus the updates would have no effect. Only an update along a transition into the state just prior to the goal, or from it, will change any values. If simulated transitions are generated uniformly, then many wasteful updates will be made before stumbling onto one of these useful ones. As planning progresses, the region of useful updates grows, but planning is still far less effcient than it would be if focused where it would do the most good. In the much larger problems that are our real objective, the number of states is so large that an unfocused search would be extremely inefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Chapter 13 Policy Approximation and its Advantages</span>\n",
    "Methods that learn approximations to both policy and value functions are often called actor–critic methods, where ‘actor’ is a reference to the learned policy, and ‘critic’ refers to the learned value function, usually a state-value function.\n",
    "\n",
    "The performance of episodic cases is defined as the value of the start state under the parameterized policy, while the performance of continuing cases is defined as the average reward rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:green\">13.2 The Policy Gradient Theorem</span>\n",
    "With continuous policy parameterization the action probabilities change smoothly as a function of the learned parameter, whereas in \"$\\epsilon$-greedy selection the action probabilities may change dramatically for an arbitrarily small change in the estimated action values, if that change results in a different action having the maximal value. Largely because of this, stronger convergence guarantees are available for policy-gradient methods than for action-value methods. In particular, it is the continuity of the policy dependence on the parameters that enables policy-gradient methods to approximate gradient ascent.\n",
    "\n",
    "Policy Gradient Theorem:\n",
    "$$\\triangledown J(\\theta)\\propto\\sum_s\\mu(s)\\sum_a q_\\pi(s,a)\\triangledown_\\pi(a|s,\\theta)$$\n",
    "\n",
    "In the episodic case, the constant of proportionality is the average length of an episode, and in the continuing case it is 1, so that the relationship is actually an equality. The distribution $\\mu$ is the on-policy distribution under $\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:green\">13.3 REINFORCE: Monte Carlo Policy Gradient</span>\n",
    "Stochastic Gradient Ascent:\n",
    "$$\\triangledown J(\\theta)\\propto\\sum_s\\mu(s)\\sum_a q_\\pi(s,a)\\triangledown\\pi(a|s,\\theta)$$\n",
    "$$=E_\\pi(\\sum_a q_\\pi(S_t,a)\\triangledown\\pi(a|S_t,\\theta))$$\n",
    "$$\\theta_{t+1}=\\theta_t+\\alpha\\sum_a q_\\pi(S_t,a)\\triangledown\\pi(a|S_t,\\theta)$$\n",
    "REINFORCE:\n",
    "$$E_\\pi(\\sum_a q_\\pi(S_t,a)\\triangledown\\pi(a|S_t,\\theta))$$\n",
    "$$=E_\\pi(\\sum_a\\pi(a|s,\\theta)q_\\pi(S_t,a)\\frac{\\triangledown\\pi(a|S_t,\\theta)}{\\pi(a|S_t,\\theta)})$$\n",
    "$$=E_\\pi(q_\\pi(S_t,A_t)\\frac{\\triangledown\\pi(A_t|S_t,\\theta)}{\\pi(A_t|S_t,\\theta)})$$\n",
    "$$=E_\\pi(G_t\\frac{\\triangledown\\pi(A_t|S_t,\\theta)}{\\pi(A_t|S_t,\\theta)})$$\n",
    "$$=E_\\pi(G_t\\triangledown ln\\pi(A_t|S_t,\\theta))$$\n",
    "$\\triangledown ln\\pi(a|S_t,\\theta))$ is referred to as Eligibility Vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:green\">13.4 REINFORCE with Baseline</span>\n",
    "The policy gradient theorem can be generalized to include a comparison of the action value to an arbitrary baseline $b(s)$:\n",
    "$$\\triangledown J(\\theta)\\propto\\sum_s\\mu(s)\\sum_a(q_\\pi(s,a)-b(s))\\triangledown\\pi(a|s,\\theta)$$\n",
    "The baseline can be any function, even a random variable, as long as it does not vary with a; the equation remains valid because the subtracted quantity is zero\n",
    "\n",
    "The policy gradient theorem with baseline can be used to derive an update rule using similar steps as in the previous section.\n",
    "$$\\theta_{t+1}=\\theta_t+\\alpha(G_t-b(S_t))\\triangledown ln\\pi(A_t|S_t,\\theta_t)$$\n",
    "The natural choice for the baseline is an estimate of the state value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:green\">13.5 Actor-Critic Methods</span>\n",
    "One-step Actor-Critic:\n",
    "$$\\theta_{t+1}=\\theta_t+\\alpha(R_{t+1}+\\gamma\\hat{v}(S_{t+1},w)-\\hat{v}(S_t,w))\\triangledown ln\\pi(A_t|S_t,\\theta_t)$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
