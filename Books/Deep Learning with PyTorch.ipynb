{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">Deep Learning with PyTorch</span>\n",
    "Authors: Eli Stevens, Luca Antiga"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Chapter 1 Introducing deep learning and the PyTorch library</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:green\">1.4 PyTorch has the batteries included</span>\n",
    "To bypass the cost of the Python interpreter and offer the opportunity to run models independently from a Python runtime, PyTorch also provides a deferred execution model named TorchScript. Using TorchScript, PyTorch can serialize a set of instructions that can be invoked independently from Python. Besides not incurring the costs of calling into Python, this execution mode gives PyTorch the opportunity to Just in Time (JIT) transform sequences of known operations into more efficient fused operations. These features are the basis of the production deployment capabilities of PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Chapter 2 It starts with a tensor</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:green\">2.1 Tensor fundamentals</span>\n",
    "Using Python lists to store the vector can be suboptimal.\n",
    "- Numbers in Python are full-fledged objects. Whereas a floating-point number might take only 32 bits to be represented on a computer, Python boxes them in a full-fledged Python object with reference counting and so on. This situation isn’t a problem if you need to store a small number of numbers, but allocating millions of such numbers gets inefficient.\n",
    "- Lists in Python are meant for sequential collections of objects. No operations are defined for, say, efficiently taking the dot product of two vectors or summing vectors. Also, Python lists have no way of optimizing the layout of their content in memory, as they’re indexable collections of pointers to Python objects (of any kind, not numbers alone). Finally, Python lists are one-dimensional, and although you can create lists of lists, again, this practice is inefficient.\n",
    "- The Python interpreter is slow compared with optimized, compiled code. Performing mathematical operations on large collections of numerical data can be must faster using optimized code written in a compiled, low-level language like C.\n",
    "\n",
    "Indexing does not allocate a new chunk of memory, because that process would be inefficient. It was instead a different view of the same underlying data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:green\">2.2 Tensors and storages</span>\n",
    "Values are allocated in contiguous chunks of memory, managed by torch.Storage instances. A storage is a one-dimensional array of numerical data. The layout of a storage is always one-dimensional, irrespective of the dimensionality of any tensors that may refer to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:green\">2.3 Size, storage offset, and strides</span>\n",
    "Accessing an element $i$, $j$ in a 2D tensor results in accessing the $storage\\_offset + stride[0] * i + stride[1] * j$ element in the storage. The offset will usually be zero; if this tensor is a view into a storage created to hold a larger tensor the offset might be a positive value. This indirection between Tensor and Storage leads some operations, such as transposing a tensor or extracting a subtensor, to be inexpensive, as they don’t lead to memory reallocations; instead, they consist of allocating a new tensor object with a different value for size, storage offset, or stride.\n",
    "\n",
    "Changing the subtensor has a side effect on the original tensor too. This effect may not always be desirable, so you can eventually clone the subtensor into a new tensor.\n",
    "\n",
    "Transposing in PyTorch isn’t limited to matrices. You can transpose a multidimensional array by specifying the two dimensions along which transposing.\n",
    "\n",
    "A tensor whose values are laid out in the storage starting from the rightmost dimension onward (moving along rows for a 2D tensor, for example) is defined as being contiguous. Contiguous tensors are convenient because you can visit them efficiently and in order without jumping around in the storage.\n",
    "\n",
    "You can obtain a new contiguous tensor from a noncontiguous one by using the con- tiguous method. The content of the tensor stays the same, but the stride changes, as does the storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:green\">2.9 The tensor API</span>\n",
    "A small number of operations exist only as methods of the tensor object. They’re recognizable by the trailing underscore in their name, such as zero_, which indicates that the method operates in-place by modifying the input instead of creating a new output tensor and returning it. Any method without the trailing underscore leaves the source tensor unchanged and returns a new tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Chapter 3 Real-world data representation with tensors</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:green\">3.1 Tabular data</span>\n",
    "Columns may contain numerical values, or labels, such as a string expressing an attribute. Therefore, tabular data typically isn’t homogeneous; different columns don’t have the same type.\n",
    "\n",
    "\n",
    "PyTorch tensors, on the other hand, are homogeneous. Other data science packages, such as Pandas, have the concept of the data frame, an object representing a data set with named, heterogenous columns. By contrast, information in PyTorch is encoded as a number, typically floating-point (though integer types are supported as well). Numeric encoding is deliberate, because neural networks are mathematical entities that take real numbers as inputs and produce real numbers as output through successive application of matrix multiplications and nonlinear functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Chapter 4 The mechanics of learning</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:green\">4.2 PyTorch’s autograd: Backpropagate all things</span>\n",
    "All PyTorch tensors have an attribute named grad, normally None. The training loop starts with a tensor with requires_grad set to True, calls the model, computes the loss, and then calls backward on the loss tensor.\n",
    "\n",
    "Calling backward leads derivatives to accumulate at leaf nodes. You need to zero the gradient explicitly after using it for parameter updates. To prevent this situation from occurring, you need to zero the gradient explicitly at each iteration. You can do so easily by using the in-place zero_ method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
